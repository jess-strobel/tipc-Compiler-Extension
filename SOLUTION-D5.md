To choose optimizations for this deliverable, our group analyzed the existing Optimizer.cpp file, the example additional optimization passes added in the “project5demo” branch of the tipc GitHub repository, and LLVM’s PassBuilder.cpp file. Looking at the first two files allowed us to gain an understanding of the types of optimizations that would improve the TIP code and how these optimizations should be implemented. PassBuilder.cpp served as a good starting point for other possible optimizations and their implementations. We used the “Transforms” folder in LLVM’s GitHub repository to see the various potential optimizations we could include, grouped by category. Because example loop optimizations had already been implemented in Optimizer.cpp, we decided to begin our optimization extension by adding additional loop optimization passes. We specifically looked in the “Scalar” folder (llvm-project/llvm/include/llvm/Transforms/Scalar/) of the LLVM GitHub repository to see the loop optimization passes we could choose from. We then moved on to implementing interprocedural optimizations (in the “IPO” folder of the repository) and optimization passes for each function (also in the “Scalar” folder).

The first optimization pass we chose to include is a loop unrolling pass (header file: LoopUnrollPass.h). This was chosen because we had discussed loop unrolling previously in class, and because it appeared to be an optimization that would speed up the runtime of TIP programs due to the existence of while loops in TIP. The loop unroll optimization pass determines if it is profitable to unroll a loop (meaning, loop unrolling will decrease the time it takes for a TIP program to run) and if so, the loop is unrolled. Loop unrolling decreases the amount of iterations a loop must make, thus decreasing overall execution time. 

The next optimization pass chosen is loop unrolling and jamming (header file: LoopUnrollAndJamPass.h). This was chosen to not only support loop unrolling, as discussed above, but to account for the potential of nested loops in TIP. Loop jamming combines nested loops into a single loop in order to reduce the time needed to execute both loops.

The third optimization pass chosen is the induction variable simplification pass (header file: IndVarSimplify.h). This was chosen to simplify the induction variables used in TIP while loops and to canonicalize the loop’s exit condition (if the loop’s trip count is known at compile time) so that the loops can be further analyzed and transformed by later optimization passes. 

The fourth optimization pass chosen is block extraction (header file: BlockExtractor.h). This pass was chosen to fulfill the interprocedural optimization pass requirement and to reduce redundant code by extracting basic blocks into their own functions that can be called at runtime.

The final optimization pass chosen is sparse conditional constant propagation (header file: SCCP.h). This pass assumes the values are constant unless proven otherwise, and assumes that BasicBlocks are dead unless proven otherwise. This allows the values of variables and expressions that will always evaluate to a fixed value to be computed at compile time rather than runtime. The pass relies on SSA form in order to improve efficiency. Because of the possibility of constant values and expressions, as well as the prevalence of basic blocks in TIP code, we concluded that this optimization would assist us in decreasing program execution time.

The biggest challenge our team faced was demonstrating the benefit of our chosen optimizations. Although conceptually we understand why a given optimization will speed up program runtime (as explained above), our various attempts at creating microbenchmarks showing this speedup did not result in a significant difference in the efficiency of our TIP programs. Our team built the microbenchmark programs off of the examples in the “project5demo” branch of the tipc GitHub repository, and we also consulted examples of optimizations online to gain a better understanding of how to demonstrate such optimizations. For example, we referenced example programs that showed how loop unrolling works in order to attempt to create a microbenchmark that reflected the benefits of loop unrolling. We also used Piazza to ask and review existing questions that would help us during this challenge. 

Ultimately, we were unable to demonstrate the benefit of our chosen optimizations through program runtime. However, an analysis of the human-readable LLVM bitcode generated by compiling our microbenchmarks with and without each optimization demonstrates that the optimization was applied to our microbenchmark programs in the way that was intended. 

We compiled the microbenchmark for our loop unrolling pass without the optimization (./bin/build.sh -asm unroll.tip) as well as with the optimization (./bin/build.sh -asm -unroll unroll.tip) using the -asm flag to generate a .ll file that would allow us to analyze the LLVM bitcode of the program with and without the optimization. The bitcode for the program without the optimization demonstrates the simple while loop with a single label for the entire loop body. The bitcode for the program with the optimization, however, shows an unrolled loop with a different label for each iteration of the loop. This demonstrates the space-time tradeoff that occurs with loop unrolling, where a program’s binary size increases so that its execution speed can decrease. 

The differences observed in the bitcode for the optimized and unoptimized programs for unroll.tip were also seen in the other microbenchmarks we created for our additional optimization passes. Although we had trouble showing a change in execution time, the bitcode changes allow us to conclude that our optimizations are being applied to our code and are improving its efficiency. 

The five optimization passes described in this writeup are the ones that we selected as our final optimization passes for this deliverable. Note that there are additional optimization passes included in the project. These optimizations were kept to show our other attempts at implementing various other optimization passes, but are not intended to be used at compile time.
